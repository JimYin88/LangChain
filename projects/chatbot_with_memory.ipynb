{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af6609f-dbec-4a64-861b-3e8399f26bd8",
   "metadata": {},
   "source": [
    "# ChatBot with Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca085f-15e5-4c67-b6aa-1786818ac1b8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ca2195-afa3-4cf2-81ea-b90d097d5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "os.environ['USER_AGENT'] = 'JimYin88'\n",
    "\n",
    "# LLM Models\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Templates\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Document Loaders\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain.document_loaders import BSHTMLLoader\n",
    "# from langchain_community.document_loaders import UnstructuredRTFLoader\n",
    "\n",
    "# Document Splitters\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Vector Stores\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "# LangChain Chains\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "# OutputParsers\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Gradio\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9fc584-1382-4b44-a467-0150d179e273",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69bd42f6-2d64-4eb0-85aa-23d1dd39ee62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7652c783-ed61-468e-a14f-90312192e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "def get_docs(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = TokenTextSplitter(chunk_size=200,\n",
    "                                      chunk_overlap=20)\n",
    "\n",
    "    splitDocs = text_splitter.split_documents(docs)\n",
    "\n",
    "    return splitDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca444bc-c5f2-4f90-8dd6-dcbb0c90b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(docs):\n",
    "    embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = FAISS.from_documents(documents=docs, \n",
    "                                        embedding=embedding_function)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c6af17-669d-4cd9-a663-9f7443e9562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(vector_store):\n",
    "    \n",
    "    chat_model = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\",\n",
    "                            max_completion_tokens=1028,\n",
    "                            temperature=0.2)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", \"Answer the user's questions based on the context: {context}\"),\n",
    "                                               MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                                               (\"user\", \"{input}\")])\n",
    "\n",
    "    # chain = prompt | model\n",
    "    document_chain = create_stuff_documents_chain(llm=chat_model, \n",
    "                                                  prompt=prompt, \n",
    "                                                  output_parser=StrOutputParser())\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "    retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")])\n",
    "    \n",
    "    history_aware_retriever = create_history_aware_retriever(llm=chat_model,\n",
    "                                                             retriever=retriever,\n",
    "                                                             prompt=retriever_prompt)\n",
    "\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        # retriever, Replace with History Aware Retriever\n",
    "        history_aware_retriever,\n",
    "        document_chain)\n",
    "\n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7666ef91-7b4a-4bb1-a7b9-ae9b3fffee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chat(chain, question, chat_history):\n",
    "    \n",
    "    response = chain.invoke({\"input\": question,\n",
    "                             \"chat_history\": chat_history})      \n",
    "    \n",
    "    return response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adb3b845-14a1-41fe-a76f-c3bfbd04e98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is LCEL?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: LCEL stands for LangChain Expression Language. It is designed to provide consistency around behavior and customization over legacy subclasses within the LangChain framework. LCEL allows for the automatic conversion of functions into RunnableLambda objects, enabling the creation of chains that can process inputs in a structured manner. If you have any specific questions about LCEL or its features, feel free to ask!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Your name is Jim.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    docs = get_docs('https://python.langchain.com/docs/expression_language/')\n",
    "    vectorStore = create_vector_store(docs)\n",
    "    chain = create_chain(vectorStore)\n",
    "\n",
    "    chat_history = [HumanMessage(content='Hello.'),\n",
    "                    AIMessage(content='Hello. How can I assist you?'),\n",
    "                    HumanMessage(content='My name is Jim.')]\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        response = process_chat(chain, user_input, chat_history)\n",
    "        chat_history.append(HumanMessage(content=user_input))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "        print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e1d3b-1faa-4146-b7e5-dd1533bd2e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
